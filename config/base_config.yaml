# Base configuration for Hyper-Efficient RL Mathematical Reasoning
# This file contains default settings that can be overridden by specific configs

# Model configuration
model:
  name: "Qwen/Qwen3-4B"
  max_length: 2048
  temperature: 0.7
  use_peft: true
  lora_config:
    r: 64
    lora_alpha: 128
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# Training configuration
training:
  batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  num_epochs: 3
  warmup_steps: 100
  save_steps: 500
  logging_steps: 10
  evaluation_steps: 500

# RL specific configuration
rl:
  ppo_epochs: 4
  mini_batch_size: 4
  clip_range: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  max_grad_norm: 1.0

# Curriculum learning (SPEED)
curriculum:
  enabled: true
  difficulty_estimation_method: "zero_shot"
  initial_difficulty: 0.3
  difficulty_increment: 0.1
  max_difficulty: 1.0

# Length-aware reward configuration
reward:
  length_penalty_weight: 0.1
  max_reasoning_length: 256
  correctness_weight: 1.0

# Data configuration
data:
  datasets: ["gsm8k", "math"]
  train_split: "train"
  eval_split: "test"
  max_samples: 10000
  preprocessing:
    filter_difficulty: true
    min_steps: 2
    max_steps: 10

# Evaluation configuration
evaluation:
  benchmarks: ["gsm8k", "math", "svamp"]
  metrics: ["accuracy", "avg_token_count", "inference_latency"]
  save_predictions: true

# Hardware and environment
hardware:
  device: "auto"
  mixed_precision: true
  gradient_checkpointing: true
  dataloader_num_workers: 4

# Logging and monitoring
logging:
  use_wandb: true
  project_name: "hyper-efficient-rl-math"
  log_dir: "logs"
  save_checkpoints: true
  checkpoint_dir: "checkpoints"
